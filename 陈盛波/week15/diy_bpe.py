# æ‰©å±•è¯è¡¨
expand_vocab_1 = {} # key:åˆå¹¶åçš„tokenï¼Œvalue:åˆå¹¶å‰çš„token
expand_vocab_2 = {} # key:åˆå¹¶å‰çš„tokenï¼Œvalue:åˆå¹¶åçš„token

# åŠ è½½æ–‡æœ¬æ•°æ®
def load_data(path):
    with open(path, 'r', encoding="utf8") as f:
        content = f.read()
        return content

# åˆå¹¶ç¼–ç å¹¶å°†æ–°çš„ç¼–ç åŠ å…¥åˆ°è¯è¡¨å½“ä¸­
def merge(codes, merge_code, code):
    new_codes = list()
    i = 0
    while i < len(codes):
        merge_codes = codes[i:i+2]
        if len(merge_codes) == 2 and (merge_codes[0], merge_codes[1]) == merge_code:
            new_codes.append(code)
            i+=2
        else:
            new_codes.append(merge_codes[0])
            i+=1

    # print(new_codes)
    # print(f'codes length: {len(codes)}, new_codes length: {len(new_codes)}')
    print(f"merging {merge_code} into a new token {code}")
    expand_vocab_1.update({code: merge_code})
    expand_vocab_2.update({merge_code: code})
    return new_codes

# è·å–å½“å‰éœ€è¦åˆå¹¶çš„ç¼–ç 
def get_merge_code(ids):
    statistics = {}
    for j in range(len(ids)-1):
        merge_codes = ids[j:j+2]
        key = (merge_codes[0], merge_codes[1])
        statistics.update({key: statistics.get(key, 0) + 1})

    # print(sorted(((v,k) for k,v in statistics.items()), reverse=True))

    merge_code = max(statistics, key=statistics.get)
    return merge_code


def get_statistics_vocab(ids, num_merges):
    for i in range(num_merges):
        
        merge_code = get_merge_code(ids)
        # print(merge_code)

        ids = merge(ids, merge_code, 256+i)

    # merge([1,2,3,2,3,2,3,4], (2,3), 10)

    print(f'codes length: {len(utf8_codes)}, new_codes length: {len(ids)}')
    print(f"compression ratio: {len(utf8_codes) / len(ids):.2f}X")
    print(f'expand vocab 1: {expand_vocab_1}')
    print(f'expand vocab 2: {expand_vocab_2}')

# æ ¹æ®æ–°çš„è¯è¡¨ç»™æ–‡æœ¬ç¼–ç 
def diy_encode(text:str):
    tokens = list(text.encode("utf-8"))
    i=0
    while i < len(tokens)-1:
        key = (tokens[i], tokens[i+1])
        if key in expand_vocab_2.keys():
            tokens = tokens[0:i] + [expand_vocab_2.get(key)] + tokens[i+2:]
        else:
            i+=1

    return tokens

# æ ¹æ®æ–°çš„è¯è¡¨ç»™æ–‡æœ¬è§£ç 
def diy_decode(tokens:list):
    i=0
    while i < len(tokens):
        token = tokens[i]
        if token in expand_vocab_1.keys():
            origin_token = expand_vocab_1.get(token)
            tokens[i:i+1]= [origin_token[0], origin_token[1]]
        else:
            i+=1            

    return tokens


if __name__ == "__main__":
    # åŠ è½½æ–‡æœ¬æ•°æ®
    text = load_data("./corpus")
    print(text)
    # text = "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception."
    utf8_codes = list(text.encode("utf-8"))

    ids = list(utf8_codes)
    # # print(utf8_codes)

    vocab_size = 300 # the desired final vocabulary size  è¶…å‚æ•°ï¼šé¢„æœŸçš„æœ€ç»ˆè¯è¡¨å¤§å°ï¼Œæ ¹æ®å®é™…æƒ…å†µè‡ªå·±è®¾ç½®ï¼Œå¤§çš„è¯è¡¨ä¼šéœ€è¦å¤§çš„embeddingå±‚
    num_merges = vocab_size - 256
    # ç»Ÿè®¡è¯è¡¨
    get_statistics_vocab(ids, num_merges)

    # æµ‹è¯•ç¼–ç å‰çš„å­—ç¬¦ä¸²ä¸è§£ç åçš„å­—ç¬¦ä¸²æ˜¯å¦ä¸€è‡´
    index = 1
    for origin_str in ["hello world", "Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters."]:
        # æ ¹æ®æ–°çš„è¯è¡¨ç¼–ç 
        encode_tokens = diy_encode(origin_str)
        # print(f'diy_encode: {encode_tokens}')

        # æ ¹æ®æ–°çš„è¯è¡¨è§£ç 
        decode_tokens = diy_decode(encode_tokens)
        # print(f'diy_decode: {decode_tokens}')

        valid_str = bytes(decode_tokens).decode("utf-8")
        print(f'example {index}: valid result: {origin_str==valid_str}')
        index+=1

    # diy_decode([283, 100, 275, 256])